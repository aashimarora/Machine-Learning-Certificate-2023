{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random     \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "(Examples inspired from Section 5.2 of  ``` James, G., Witten, D., Hastie, T. & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: Springer.```)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Motivation\n",
    "Let us have two different stocks: $X$ and $Y$. Let they be normally distributed with the same mean such that $\\mu_X := \\mathbb{E}[X] = 2$ and $\\mu_Y := \\mathbb{E}[Y] = 2$. Furthermore, let $\\sigma^2_X := \\mathbb{V}\\mathrm{AR}[X] = 1$, $\\sigma^2_Y := \\mathbb{V}\\mathrm{AR}[Y] = 1.25$, and $\\sigma_{XY} := \\mathbb{C}\\mathrm{OV}[X,Y] = 0.5$.\n",
    "\n",
    "**Question 1:** How can we sample $1,000$ returns from these stocks?\n",
    "\n",
    "**Answer:** See the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "# vector of means\n",
    "mu = np.array([2 ,2])\n",
    "\n",
    "# Described covariance matrix\n",
    "r = np.array([\n",
    "        [  1,    0.5],\n",
    "        [ 0.5,  1.25]\n",
    "    ])\n",
    "\n",
    "# Generate the random samples.\n",
    "returns = np.random.multivariate_normal(mu, r, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the returns\n",
    "x = returns[:,0]\n",
    "y = returns[:,1]\n",
    "x1, y1 = [-1, 5.5], [-1, 5.5]\n",
    "plt.plot(x1, y1, c = \"r\", alpha= .3)\n",
    "plt.title(r\"Returns of stocks $X$ and $Y$, points below the red line are the cases where $X$ has a larger return\") \n",
    "plt.xlabel(r\"$X$\") \n",
    "plt.ylabel(r\"$Y$\") \n",
    "plt.scatter(x,y,s=2) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Let us invest a money on stocks $X$ and $Y$. We will invest a portion $\\alpha \\in [0,1]$ of our money on stock $X$, and the remaining $1 - \\alpha$ on stock $Y$. Our goal is to minimize the risk of this investment, where we measure risk simply via variance in this case. In other words, we would like to solve:\n",
    "$$\\alpha^\\star \\in \\arg\\min_{\\alpha} \\{ \\mathbb{V}\\mathrm{AR}[\\alpha X + (1-\\alpha)Y] \\}$$\n",
    "How can find such an optimal $\\alpha^\\star$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Recall that \n",
    "\\begin{align*} \\mathbb{V}\\mathrm{AR}[\\alpha X + (1-\\alpha)Y] &= \\alpha^2 \\mathbb{V}\\mathrm{AR}[X] + (1-\\alpha)^2 \\mathbb{V}\\mathrm{AR}[Y] + 2\\alpha(1-\\alpha) \\mathbb{C}\\mathrm{OV}[X,Y]\\\\ \n",
    "& = \\alpha^2 \\sigma^2_X + (1-\\alpha)^2 \\sigma^2_Y + 2\\alpha(1-\\alpha) \\sigma_{XY}\n",
    "\\end{align*}\n",
    "Let us minimize this function by plugging in the true values of $\\sigma_X^2 = 1,\\  \\sigma_Y^2 = 1.25, \\ \\sigma_{XY} = 0.5$ by using ```scipy optimize```. When we plug these values our problem is:\n",
    "\\begin{align*}\n",
    "\\begin{array}{ll}\n",
    "\\mathrm{minimize}_\\alpha & (1-\\alpha)^2 1.25 + \\alpha \\\\\n",
    "\\mathrm{subject to} & \\alpha \\in [0,1].\n",
    " \\end{array}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_given(alpha):\n",
    "    \"\"\"The objective function to minimize\"\"\"\n",
    "    return (1-alpha)**2 * 1.25 + alpha\n",
    "bounds = optimize.Bounds([0],[1])\n",
    "solution = optimize.minimize(objective_given, x0 = [0], bounds = bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal solution is when alpha =  0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal solution is when alpha = \", round(float(solution.x),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we keep $\\sigma^2_X, \\sigma^2_Y, \\sigma_{XY}$ as parameters and change them whenever we want? So we want to directly solve\n",
    "\\begin{align*}\n",
    "\\begin{array}{ll}\n",
    "\\mathrm{minimize}_\\alpha & \\alpha^2 \\sigma^2_X + (1-\\alpha)^2 \\sigma^2_Y + 2\\alpha(1-\\alpha) \\sigma_{XY} \\\\\n",
    "\\text{subject to} & \\alpha \\in [0,1].\n",
    " \\end{array}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_given(alpha, varx, vary, covar):\n",
    "    \"\"\"The objective function to minimize\"\"\"\n",
    "    return (alpha**2 * varx) + ((1-alpha)**2 * vary) + (2*alpha*(1-alpha)*covar)\n",
    "bounds = optimize.Bounds([0],[1])\n",
    "solution = optimize.minimize(objective_given, x0 = [0], bounds = bounds, args=(1,1.25,0.5)) #this was previous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal solution is when alpha =  0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal solution is when alpha = \", round(float(solution.x),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal solution is when alpha =  0.143\n"
     ]
    }
   ],
   "source": [
    "solution = optimize.minimize(objective_given, x0 = [0], bounds = bounds, args=(5,1.25,0.5))\n",
    "print(\"The optimal solution is when alpha = \", round(float(solution.x),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the larger $\\sigma^2_X$ gets, the less we invest in $X$ as $\\alpha$ decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common error in optimization practice is that in several cases we don't need to use an optimization solver, and derive the solution ourselves. If we can do that, one particular advantage would be, the structure of the optimal solution will be parametrized by $\\sigma_X^2, \\ \\sigma_Y^2, \\ \\sigma_{XY}$. This will, in turn, allow us to have more intuition on the optimal investment strategy. Let's see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the objective function:\n",
    "\\begin{align*} \\mathbb{V}\\mathrm{AR}[\\alpha X + (1-\\alpha)Y] &= \\alpha^2 \\sigma^2_X + (1-\\alpha)^2 \\sigma^2_Y + 2\\alpha(1-\\alpha) \\sigma_{XY} \\\\\n",
    "& = \\alpha^2 \\sigma^2_X + \\sigma^2_Y + \\alpha^2 \\sigma^2 Y - 2\\alpha \\sigma^2_Y + 2\\alpha \\sigma_{XY} - 2\\alpha^2 \\sigma_{XY} \\\\\n",
    "& = \\alpha^2(\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}) + 2\\alpha(\\sigma_{XY} - \\sigma^2_Y) + \\sigma_Y^2\n",
    "\\end{align*}\n",
    "Notice that this is a quadratic function of $\\alpha$, and the coefficient of $\\alpha^2$ is $\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}$ which is $\\mathbb{V}\\mathrm{AR}[X - Y] \\geq 0$. Hence, $\\alpha^2$ has a coefficient term, concluding the objective function is **convex**. So if we take the first order conditions, then the solution that solves them is optimal. In other words:\n",
    "\\begin{align*}\n",
    "\\dfrac{\\mathrm{d}\\left[ \\alpha^2(\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}) + 2\\alpha(\\sigma_{XY} - \\sigma^2_Y) + \\sigma_Y^2 \\right]}{\\mathrm{d} \\alpha} = 2\\alpha (\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}) + 2(\\sigma_{XY} - \\sigma^2_Y)\n",
    "\\end{align*}\n",
    "Optimal $\\alpha^\\star$ sets the above expression equal to zero, hence it immediately follows that:\n",
    "\\begin{align*}\n",
    "\\alpha^\\star = \\dfrac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n",
    "\\end{align*}\n",
    "Let's test if this is correct now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimal_alpha(varx, vary, covar):\n",
    "    return (float) (vary - covar)/(varx + vary - 2*covar)\n",
    "round(optimal_alpha(1, 1.25, 0.5),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.143"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(optimal_alpha(5, 1.25, 0.5),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It is giving the previously optimized solutions. Please let us know your interpretation of this optimal investment strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** In real life, we **never** know the distribution of stocks $X$ and $Y$. Hence we can only **estimate** $\\sigma^2_X, \\ \\sigma^2_Y, \\ \\sigma_{XY}$ from a sample of $(X,Y)$ realizations. Let us have $n$ samples of $(X,Y)$, name them $(X_i, Y_i)$ for $i=1,\\ldots,n$. So how can we decide the optimal investment strategy of the future, by looking at the **past data**? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We estimate $\\sigma_X^2$, $\\sigma_Y^2$, $\\sigma_{XY}$ by using the sample we are given. Let these estimates be $\\hat{\\sigma}_X^2, \\ \\hat{\\sigma}_Y^2, \\ \\hat{\\sigma}_{XY}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Estimate the mean returns $\\bar{X}, \\ \\bar{Y}$ by*\n",
    "\\begin{align*}\n",
    "\\bar{X} &= \\frac{1}{n}\\sum_{i=1}^n X_i \\\\\n",
    "\\bar{Y} &= \\frac{1}{n}\\sum_{i=1}^n Y_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.95247227 1.97292093]\n"
     ]
    }
   ],
   "source": [
    "hat_mean = np.mean(returns,0)\n",
    "print(hat_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97695395, 0.50269434],\n",
       "       [0.50269434, 1.30685668]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hat_cov = np.dot((returns - hat_mean).T, (returns - hat_mean))\n",
    "hat_cov = hat_cov/(n-1)\n",
    "hat_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Estimate the covariance matrix by*\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\sigma}^2_X & \\hat{\\sigma}_{XY} \\\\\n",
    "\\hat{\\sigma}_{XY} & \\hat{\\sigma}^2_Y\n",
    "\\end{bmatrix} = \\dfrac{1}{n-1} \\sum_{i=1}^n \\left( \\begin{bmatrix} X_i & Y_i \\end{bmatrix}^\\top \\begin{bmatrix} X_i & Y_i \\end{bmatrix} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97695395, 0.50269434],\n",
       "       [0.50269434, 1.30685668]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hat_cov = np.zeros((2,2))\n",
    "for i in range(n):\n",
    "    hat_cov = hat_cov + (returns[i]-hat_mean).reshape((2,1))*(returns[i]-hat_mean).reshape((1,2))\n",
    "hat_cov = hat_cov/(n-1)\n",
    "hat_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_varx = hat_cov[0,0]\n",
    "hat_vary = hat_cov[1,1]\n",
    "hat_covar = hat_cov[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3: Estimate $\\alpha^\\star$ by*\n",
    "\\begin{align*}\n",
    "\\hat{\\alpha}^\\star = \\dfrac{\\hat{\\sigma}^2_Y - \\hat{\\sigma}_{XY}}{\\hat{\\sigma}^2_X + \\hat{\\sigma}^2_Y - 2\\hat{\\sigma}_{XY}}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(optimal_alpha(hat_varx, hat_vary, hat_covar),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real optimal strategy (if we really knew the distribution behind $(X,Y)$) would be $\\alpha^\\star = 0.6$, however, as we see $1,000$ samples, we can estimate $0.629$. Let us make a function that generalizes this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n, varx=1, vary=1.25, covar=0.5): #generate a sample of n returns\n",
    "    mu = np.array([2 ,2])\n",
    "    r = np.array([\n",
    "            [  varx, covar],\n",
    "            [ covar,  vary]])\n",
    "    sample_returns = np.random.multivariate_normal(mu, r, size=n)\n",
    "    return sample_returns\n",
    "def hat_alpha(sample_returns): #estimate the optimal investment strategy\n",
    "    hat_mean = np.mean(sample_returns,0)\n",
    "    hat_cov = np.zeros((2,2))\n",
    "    n = np.size(sample_returns,0)\n",
    "    for i in range(n):\n",
    "        hat_cov = hat_cov + (sample_returns[i]-hat_mean).reshape((2,1))*(sample_returns[i]-hat_mean).reshape((1,2))\n",
    "    hat_cov = hat_cov/(n-1)\n",
    "    hat_varx = hat_cov[0,0]\n",
    "    hat_vary = hat_cov[1,1]\n",
    "    hat_covar = hat_cov[0,1]\n",
    "    return round(optimal_alpha(hat_varx, hat_vary, hat_covar),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** How does our estimation change if we see different samples? Can it be that our estimation $\\hat{\\alpha}^\\star$ is very different than the real $\\alpha$ (which we don't see), but the specific sample of returns give us a huge bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = 1000 #number of times to simulate\n",
    "samples = 1000 #how many samples do we see in each \"scenario\"\n",
    "estimations = np.zeros(simulation)\n",
    "for sim in range(simulation):\n",
    "    estimations[sim] = hat_alpha(generate_sample(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.601 min 0.528 max 0.686\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\", round(np.mean(estimations),3),\"min\",np.min(estimations), \"max\", np.max(estimations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean of all estimations are equal to the true $\\alpha$, which is the result of our unbiased estimation. However, our estimation vary between $[0.528, 0.686]$. How often do you think we have 'bad' values? Let's see in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(estimations,  bins = 20)  # density=False would make counts\n",
    "plt.axvline(x=0.6, color='r', linestyle='--', label = r\"true value of $\\alpha^\\star$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we had $1,000$ simulations, and in each simulation we had $1,000$ samples. In real-life, we rarely get such a big sample. What if we just have $100$ samples? Let's see the histogram again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.595 min 0.359 max 0.87\n"
     ]
    }
   ],
   "source": [
    "samples = 100 #how many samples do we see in each \"scenario\"\n",
    "estimations = np.zeros(simulation)\n",
    "for sim in range(simulation):\n",
    "    estimations[sim] = hat_alpha(generate_sample(samples))\n",
    "print(\"mean\", round(np.mean(estimations),3),\"min\",np.min(estimations), \"max\", np.max(estimations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(estimations,  bins = 30, color = 'g')  # density=False would make counts\n",
    "plt.axvline(x=0.6, color='r', linestyle='--', label = r\"true value of $\\alpha^\\star$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** In real life we are in a single 'simulation'. As we don't know the real distribution of data, we cannot generate $1,000$ samples and decide accordingly. We need to take the risk and trust on our $\\hat{\\alpha}^\\star$ estimation. The question is, can we quantify this 'trust'? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It turns out yes!!! Bootstrapping saves us here. Let us first see simple statistics of our previous simulation. As a step 1, let us denote the previous simulations by $B = 1,\\ldots,1000$. Recall that in the very last experiment, in every simulation we generated a sample of $100$ returns of $(X,Y)$. What is the mean of our estimations $\\hat{\\alpha}_b$ (where $b \\in \\{ 1,\\ldots, B\\}$ is the simulation number)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get that via $\\bar{\\alpha} = \\sum_{b=1}^B \\hat{\\alpha}_b$. Note that we drop the notation $\\alpha^\\star$ as it is clear that we are estimating the optimal $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.595"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = simulation #notational convenience\n",
    "bar_alpha = np.mean(estimations)\n",
    "round(bar_alpha,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the standard deviation of these estmates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.081"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_error = np.sqrt( np.sum(np.square(estimations - bar_alpha)) /(B-1))\n",
    "round(standard_error,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that our estimation satisfies $SE(\\hat{\\alpha}) \\approx 0.081$ **based on the simulations**. So if we randomly sample $100$ returns of $(X,Y)$ just once, and estimate $\\hat{\\alpha}$ via the formula we derived, i.e.,\n",
    "\\begin{align*}\n",
    "\\hat{\\alpha}^\\star = \\dfrac{\\hat{\\sigma}^2_Y - \\hat{\\sigma}_{XY}}{\\hat{\\sigma}^2_X + \\hat{\\sigma}^2_Y - 2\\hat{\\sigma}_{XY}}.\n",
    "\\end{align*}\n",
    "then we expect this number to deviate from the *true optimal* $\\alpha$, i.e., \n",
    "\\begin{align*}\n",
    "\\alpha^\\star = \\dfrac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}}.\n",
    "\\end{align*}\n",
    "by an amount of $0.082$ in average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the above experiments, we simulated a sampe of returns (of size $100$) for $1,000$ times. Such sampling requires us to use the distribution of $(X,Y)$. Again, in real life, we do **not** know the true distribution. We only have a single sample of $100$ returns of $(X,Y)$ and that is all we know. Let us denote the sample as $\\mathcal{R} = \\{(X_i, Y_i) \\ : \\ i = 1,\\ldots,100 \\}$.\n",
    "\n",
    "So how can we apply the previous simulation? Here comes bootstrapping. The idea is simple: as before, we simulate $1,000$ times, and in each simulation we sample $100$ realizations of $(X,Y)$ returns. But, this time, we use the returns $\\mathcal{R}$ we already have. However, if we sample $100$ distinct values, this is basically equivalent with the true set of returns we have. Instead, here, we sample **with replacements**. Let's see how it works now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: We are given a sample of $100$ returns and we do not know the true distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_returns = np.random.multivariate_normal(mu, r, size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: We estimate the optimal investment strategy by estimating $\\hat{\\alpha}$ from the given returns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_alpha = hat_alpha(given_returns)\n",
    "estimated_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3: We see the standard error associated with such estimation technique by using Bootstrapping*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.651 min 0.384 max 0.912\n"
     ]
    }
   ],
   "source": [
    "simulation = 1000#same as before\n",
    "B = simulation\n",
    "samples = 100#same\n",
    "estimations = np.zeros(simulation)#same as before \n",
    "for sim in range(simulation):\n",
    "    generated_sample = given_returns[np.random.randint(given_returns.shape[0], size=samples), :]\n",
    "    estimations[sim] = hat_alpha(generated_sample)\n",
    "print(\"mean\", round(np.mean(estimations),3),\"min\",np.min(estimations), \"max\", np.max(estimations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute standard error\n",
    "bootstrap_error = np.std(estimations)\n",
    "np.round(bootstrap_error,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function var in module numpy:\n",
      "\n",
      "var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=<no value>, *, where=<no value>)\n",
      "    Compute the variance along the specified axis.\n",
      "    \n",
      "    Returns the variance of the array elements, a measure of the spread of a\n",
      "    distribution.  The variance is computed for the flattened array by\n",
      "    default, otherwise over the specified axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array containing numbers whose variance is desired.  If `a` is not an\n",
      "        array, a conversion is attempted.\n",
      "    axis : None or int or tuple of ints, optional\n",
      "        Axis or axes along which the variance is computed.  The default is to\n",
      "        compute the variance of the flattened array.\n",
      "    \n",
      "        .. versionadded:: 1.7.0\n",
      "    \n",
      "        If this is a tuple of ints, a variance is performed over multiple axes,\n",
      "        instead of a single axis or all the axes as before.\n",
      "    dtype : data-type, optional\n",
      "        Type to use in computing the variance.  For arrays of integer type\n",
      "        the default is `float64`; for arrays of float types it is the same as\n",
      "        the array type.\n",
      "    out : ndarray, optional\n",
      "        Alternate output array in which to place the result.  It must have\n",
      "        the same shape as the expected output, but the type is cast if\n",
      "        necessary.\n",
      "    ddof : int, optional\n",
      "        \"Delta Degrees of Freedom\": the divisor used in the calculation is\n",
      "        ``N - ddof``, where ``N`` represents the number of elements. By\n",
      "        default `ddof` is zero.\n",
      "    keepdims : bool, optional\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "        If the default value is passed, then `keepdims` will not be\n",
      "        passed through to the `var` method of sub-classes of\n",
      "        `ndarray`, however any non-default value will be.  If the\n",
      "        sub-class' method does not implement `keepdims` any\n",
      "        exceptions will be raised.\n",
      "    \n",
      "    where : array_like of bool, optional\n",
      "        Elements to include in the variance. See `~numpy.ufunc.reduce` for\n",
      "        details.\n",
      "    \n",
      "        .. versionadded:: 1.20.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    variance : ndarray, see dtype parameter above\n",
      "        If ``out=None``, returns a new array containing the variance;\n",
      "        otherwise, a reference to the output array is returned.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    std, mean, nanmean, nanstd, nanvar\n",
      "    :ref:`ufuncs-output-type`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The variance is the average of the squared deviations from the mean,\n",
      "    i.e.,  ``var = mean(x)``, where ``x = abs(a - a.mean())**2``.\n",
      "    \n",
      "    The mean is typically calculated as ``x.sum() / N``, where ``N = len(x)``.\n",
      "    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n",
      "    instead.  In standard statistical practice, ``ddof=1`` provides an\n",
      "    unbiased estimator of the variance of a hypothetical infinite population.\n",
      "    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n",
      "    normally distributed variables.\n",
      "    \n",
      "    Note that for complex numbers, the absolute value is taken before\n",
      "    squaring, so that the result is always real and nonnegative.\n",
      "    \n",
      "    For floating-point input, the variance is computed using the same\n",
      "    precision the input has.  Depending on the input data, this can cause\n",
      "    the results to be inaccurate, especially for `float32` (see example\n",
      "    below).  Specifying a higher-accuracy accumulator using the ``dtype``\n",
      "    keyword can alleviate this issue.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1, 2], [3, 4]])\n",
      "    >>> np.var(a)\n",
      "    1.25\n",
      "    >>> np.var(a, axis=0)\n",
      "    array([1.,  1.])\n",
      "    >>> np.var(a, axis=1)\n",
      "    array([0.25,  0.25])\n",
      "    \n",
      "    In single precision, var() can be inaccurate:\n",
      "    \n",
      "    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "    >>> a[0, :] = 1.0\n",
      "    >>> a[1, :] = 0.1\n",
      "    >>> np.var(a)\n",
      "    0.20250003\n",
      "    \n",
      "    Computing the variance in float64 is more accurate:\n",
      "    \n",
      "    >>> np.var(a, dtype=np.float64)\n",
      "    0.20249999932944759 # may vary\n",
      "    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n",
      "    0.2025\n",
      "    \n",
      "    Specifying a where argument:\n",
      "    \n",
      "    >>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n",
      "    >>> np.var(a)\n",
      "    6.833333333333333 # may vary\n",
      "    >>> np.var(a, where=[[True], [True], [False]])\n",
      "    4.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute standard error\n",
    "bootstrap_error = np.sqrt( np.sum(np.square(estimations - np.mean(estimations))) /(B-1))\n",
    "np.round(bootstrap_error,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimation of the standard error from the simulations when we knew the distribution (which is very close to the truth due to the law of large numbers) was 0.081\n",
      "While the bootstrap estimation of the standard error is 0.076\n",
      "Boostrapping does not use any single information except for the given single 100-returns!\n"
     ]
    }
   ],
   "source": [
    "print(\"The estimation of the standard error from the simulations when we knew the distribution (which is very close to the truth due to the law of large numbers) was\", round(standard_error,3))\n",
    "print(\"While the bootstrap estimation of the standard error is\", round(bootstrap_error,3))\n",
    "print(\"Boostrapping does not use any single information except for the given single 100-returns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** When we are given a single sample of $100$-returns and estimate $\\hat{\\alpha}$, how can we obtain a confidence interval of the true $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** This answer is inspired by ```Professor Martin Haugh```'s lecture notes on ```Resampling Methods```.\n",
    "Let us be interested in a $1-\\beta$-Confidence Interval (CI) on the true $\\alpha$. Fix any $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $q_l, \\ q_u$ be the $\\beta/2$ lower and upper quantiles of the bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql = np.quantile(estimations, beta/2)\n",
    "qu = np.quantile(estimations, 1- beta/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the fraction of $\\hat{\\alpha}_b$ satisfying $q_l \\leq \\hat{\\alpha}_b \\leq q_u$ out of all $b=1,\\ldots,B$ (where $B =1000$ in this case) is $1-  \\beta$. Let's observe this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect = 0\n",
    "for i in range(B):\n",
    "    if estimations[i] <= qu and estimations[i] >= ql:\n",
    "        collect += 1\n",
    "collect/B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach is to claim $\\alpha$ lies in the range $[q_l, q_u]$ which is a $1-\\beta$ ($90\\%$ in this case) confidence interval. Hence in our case (recall true $\\alpha = 0.6$), our confidence interval is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45995, 0.71025])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ql, qu])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can improve this! Recall that $\\bar{\\alpha}$ is our estimation by using the whole sample of 100 returns. It is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.594963"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bar_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimations that satisfy $q_l \\leq \\hat{\\alpha}_b \\leq q_u$ also satisfy $\\bar{\\alpha} - q_u \\leq \\bar{\\alpha} - \\hat{\\alpha}_b \\leq \\bar{\\alpha} -q_l$. Hence, $\\bar{\\alpha} - q_u$ and $\\bar{\\alpha}- q_l$ are lower/upper $\\beta/2$ quantiles for $\\bar{\\alpha} - \\hat{\\alpha}_b$!\n",
    "\n",
    "So if we add $\\bar{\\alpha}$ to this equation, we will obtain: $2\\bar{\\alpha} - q_u \\leq \\alpha \\leq 2\\bar{\\alpha}  - q_l$.\n",
    "\n",
    "Thus, a $(1 - \\beta)\\%$ CI of the true $\\alpha$ is $(2\\bar{\\alpha} - q_u, 2\\bar{\\alpha}  - q_l)$. In our case, a $90\\%$ CI is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.479676, 0.729976])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([2*bar_alpha - qu, 2*bar_alpha - ql])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that previously we observed that the minimum bootsrap estimate of the $\\alpha$ was $0.35$ and the maximum was $0.85$, so we can observe how a $90\\%$ CI is more compact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- How would you modify the estimation of $\\alpha$, the simulation, and the Bootstrapping for obtaining CI, if you were told that $(X,Y)$ has a normal distribution?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
