{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# use the following to plot SVMs\n",
    "from mlxtend.plotting import plot_decision_regions # on terminal, install: pip.install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Support Vector Machines-based classifier on a non-separable dataset\n",
    "**Load the 2-D data `Case2linear/X.npy` that has 20 rows and 2 columns, and the corresponding target `Case2linear/y.np`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20 # n points in each group\n",
    "X = np.load('Case2linear/X.npy')\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the two-dimensional points and color them so that points with the same class have the same color**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.concatenate((np.repeat(\"red\", n), np.repeat(\"blue\",n)), axis=0) #y is split half half\n",
    "plt.scatter(X[:,0], X[:,1], c = color, alpha= .5)\n",
    "plt.xlabel(...)\n",
    "plt.ylabel(...)\n",
    "plt.title(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does it look like a linear hard-margin SVM (or SVC since we are classifying) wouldn't work here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Answer: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a soft-margin linear SVC with associated cost $=1$, derive the confusion matrix, and visualise the decision boundary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well, we can still call `SVC` and this will work, because by default this package solves soft margin SVC\n",
    "clf_c1 = svm.SVC(kernel = 'linear', C = 1)\n",
    "clf_c1.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ... #fill the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the decision boundary\n",
    "with warnings.catch_warnings(): #otherwise the package might complain there is no \"boundary\" when we classify all same\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plot_decision_regions(X, y, clf=clf_c1, legend=2, colors = \"red,blue\", markers= \"o\");\n",
    "    ax=plt.gca();\n",
    "    plt.title(\"Linear decision boundary\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a soft-margin linear SVC with associated cost $=0.01$, derive the confusion matrix, and visualise the decision boundary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#well, we can still call `SVC` and this will work, because by default this package solves soft margin SVC\n",
    "clf_c001 = ...\n",
    "clf_c001.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings(): #otherwise the package might complain there is no \"boundary\" when we classify all same\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plot_decision_regions(X, y, clf=clf_c001, legend=2, colors = \"red,blue\", markers= \"o\");\n",
    "    ax=plt.gca();\n",
    "    plt.title(\"Linear decision boundary\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the number of support vectors of the previous two points**\n",
    "\n",
    "Intuitively, support vectors are the vectors deleting which will result in the decision boundary to change. Remembering the bias-variance tradeoff, we would expect the model with a smaller variance to have more support vectors, since otherwise it means that deleting small number of points change the shape of the decision boundary which would then make the model be highly biased on the training set.\n",
    "\n",
    "Moreover, in soft-margin classification, increasing the associated cost will penalize misclassification more, hence the SVC will be forced to be biased. We therefore expect the number of support vectors to decrease with increasing cost. Use `clf.support_vectors_` attribute of `sklearn.svm` to compare the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors_c1 = clf_c1.support_vectors_ ... #count the numbers (optional: mark the support vectors on the plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vectors_c1 = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
