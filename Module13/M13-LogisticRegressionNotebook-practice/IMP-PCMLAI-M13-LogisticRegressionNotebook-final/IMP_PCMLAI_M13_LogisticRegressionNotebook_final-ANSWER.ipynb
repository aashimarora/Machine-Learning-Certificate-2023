{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "D7vdQZ6FxEiR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R7jPtW2Z0iz"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we will investigate Logistic Regression on the Pima Indians Diabetes dataset (https://www.kaggle.com/uciml/pima-indians-diabetes-database) where we will try to predict the onset of Diabetes, based on certain diagnostics and measurements.\n",
    "\n",
    "Begin by downloading, and importing the data set.\n",
    "\n",
    "This will be followed by a short recap of logistic regression, and the training procedure. The first set of exercises will ask you to submit a short proof that the sigmoid function always lies between 0 and 1, and also to fit the data-set to a Logistic Regression model.\n",
    "\n",
    "We then look at choosing the best model for classification. For this, we will consider the same variable selection as seen in the first notebook. We will then have a short introduction on regularization. This is a very important concept in Machine Learning, it can be used to avoid over-fitting, as well as for variable selection (which we will not cover). The second set of exercises consists in choosing the best regularization constant, and exploring its effect on the model.\n",
    "\n",
    "Finally, we will look at *interpreting* the output of the model to make decisions. We do this by changing the probability thresholds for classification, and explain how we will choose them by considering risk of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnSu23jhOVZA"
   },
   "source": [
    "# Downloading and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "eYNRqkcqyH0M",
    "outputId": "c55aa28a-7d6e-4c78-b149-19dbe69b9d0f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0qemfflyYSb"
   },
   "source": [
    "We must then split the data into inputs and outputs. We also remove the Insulin variable, since the data is incomplete (there are ways of dealing with incomplete data, but that is not the purpose of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "U6z2SNgVyWJX"
   },
   "outputs": [],
   "source": [
    "outputs = ['Outcome']\n",
    "inputs = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "X = data[inputs]\n",
    "Y = data[outputs].to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqX_Zn3Hxhfd"
   },
   "source": [
    "We will now scale the data. Can you explain why this step needs to be done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "RdH34bJixuRh"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyPlsKPP608P"
   },
   "source": [
    "Finally, we divide it into a 80 / 20 training and testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "XrzVsMpryuxB"
   },
   "outputs": [],
   "source": [
    "num_of_points = len(Y)\n",
    "np.random.seed(10)\n",
    "idx = list(range(num_of_points))\n",
    "np.random.shuffle(idx)\n",
    "idx_train = idx[:int(num_of_points * 0.8)]\n",
    "idx_train.sort()\n",
    "idx_test = idx[int(num_of_points * 0.8):]\n",
    "idx_test.sort()\n",
    "\n",
    "X_train = X[idx_train, :]\n",
    "X_test = X[idx_test, :]\n",
    "\n",
    "Y_train = Y[idx_train]\n",
    "Y_test = Y[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAmjUYZjxPVF"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Recall that logistic regression is a model that is very well suited to problems where we have a binary output. That is, we are trying to use certain predictors to classify a particular input into one of two classes. We can write this down more formally.\n",
    "\n",
    "Assume we have a set of predictors $x \\in \\mathcal{X}$, and a set of outputs $y \\in \\{0, 1 \\}$. We are then interested in estimating the probability of belonging to a certain class, that is, we want to build an estimator, $\\hat{p}(x)$, such that:\n",
    "$$\n",
    "\\hat{p}(x) = \\mathbb{P}( Y = 1 | X = x )\n",
    "$$\n",
    "\n",
    "There are many real-world scenarios where this might be the case. Consider for example, we might be interested in predicting if a particular individual infected with COVID will need hospital treatment. In this case, we would define $Y = 1$ the scenario where the individual requires treatment, and $Y = 0$ otherwise. For predictors we could use $X = $ age. \n",
    "\n",
    "We could decide to use linear regression, that is, a model of the form:\n",
    "$$\n",
    "\\hat{p}(x) = \\beta_0 + x \\beta_1\n",
    "$$\n",
    "\n",
    "However, we run into trouble. Probabilities must lie between 0 and 1! The linear model is unable to incorporate this property into its predictions!\n",
    "\n",
    "We can instead wrap our linear model in a function that guarantees all our outputs lie between 0 and 1. We will be using the sigmoid function, defined as:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "After wrapping the linear model, we obtain the logistic regression model, given by:\n",
    "$$\n",
    "\\hat{p}(x) = \\sigma(\\beta_0 + x \\beta_1)\n",
    "$$\n",
    "\n",
    "Which we can write as:\n",
    "$$\n",
    "\\hat{p}(x) = \\frac{e^{\\beta_0 + x \\beta_1}}{1 + e^{\\beta_0 + x \\beta_1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9LRuT3WCn7J"
   },
   "source": [
    "# Training the function\n",
    "\n",
    "The parameters of a linear regression model can be estimated using least squares. However, it is not the best optimiser for logistic regression. This is because we are not estimating $Y$ directly, instead we are estimating $\\textit{the probability}$ of Y. This means maximum likelihood estimation is much more appropriate for logistic regression. We can write the likelihood function easily:\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i : y_i = 1} \\mathbb{P}(Y = 1 | X = x_i) \\prod_{i' : y_{i'} = 0} (1 - \\mathbb{P}(Y = 0 | X = x_{i'}))\n",
    "$$\n",
    "\n",
    "Instead of maximising this directly, it is more common to minimise the negative log-likelihood:\n",
    "$$\n",
    "\\ell(\\beta) = - \\log \\mathcal{L}(\\beta) = - \\sum_{i : y_i = 1} \\log\\sigma(x_i^T \\beta) - \\sum_{i' : y_{i'} = 0}\\log \\sigma(x_{i'}^T\\beta)\n",
    "$$\n",
    "So that we choose our parameters, $\\hat{\\beta}$:\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{\\beta} \\ell(\\beta)\n",
    "$$\n",
    "\n",
    "Finding the minimum can easily be done using any gradient-based optimiser. In addition, we can also add regularization, which helps us avoid overfitting. We will begin by investigating this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R90i_2UNu_0l"
   },
   "source": [
    "## Exercise 1:\n",
    "\n",
    "1. Prove that the sigmoid function always gives an output between 0 and 1. (Hint: consider the limits as $x \\rightarrow \\pm \\infty$, and show the function is always increasing).\n",
    "\n",
    "Answer:\n",
    "For $$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "we know min x = $-\\infty$ and max x = $+\\infty$\n",
    "\n",
    "Substituting those values in the expression, \n",
    "we get interval [0,1] given $\\frac{1}{1 + e^{-(-\\infty)}}$ = 0 for min x and $\\frac{1}{1 + e^{-(\\infty)}}$ = 1\n",
    "\n",
    "2. Using scikit.learn's LogisticRegression class, train a model on the data-set above, make sure you are not regularizing (read the sklearn's documentation! - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Show the model's training and test accuracy, and build a confusion matrix for each set. (Hint: the functions required are $ \\texttt{.fit()} $, $\\texttt{.predict()}$, and $\\texttt{confusion\\_matrix()}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Accuracy #################\n",
      "Training accuracy 0.7768729641693811\n",
      "Test accuracy 0.7597402597402597\n",
      "\n",
      "\n",
      "########## Confusion Matrix #################\n",
      "Confusion matrix- Train\n",
      " [[356  44]\n",
      " [ 93 121]]\n",
      "Confusion matrix- Test\n",
      " [[84 16]\n",
      " [21 33]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_test = model.predict(X_test)\n",
    "\n",
    "print ('########## Accuracy #################')\n",
    "\n",
    "print('Training accuracy', accuracy_score(Y_train, Y_pred_train))\n",
    "print('Test accuracy', accuracy_score(Y_test, Y_pred_test))\n",
    "print('\\n')\n",
    "print ('########## Confusion Matrix #################')\n",
    "print('Confusion matrix- Train\\n', confusion_matrix(Y_train, Y_pred_train))\n",
    "print('Confusion matrix- Test\\n', confusion_matrix(Y_test, Y_pred_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCvHNIle66HT"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Regularization happens when we add a penalty to the loss function. We do this to decrease the complexity of the model, in an attempt to stop the model from over-fitting. Ideally, this should lead to better generalization. To be more precise, for L2 regularization we will now choose our parameters by minimizing the modified loss function:\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_\\beta \\{ C \\cdot \\ell(\\beta) + \\frac{1}{2}\\beta^T \\beta \\}\n",
    "$$\n",
    "\n",
    "Note that the new penalization means that $\\beta$ should be closer to zero (this is what we mean by 'a simpler model').\n",
    "\n",
    "Investigate the effect of L2 regularization. In particular, focus on the how the testing accuracy changes for different values of $C$. Create a plot that shows how $C$ varies, starting at $10^{-6}$ and ending at $10^{-2}$.\n",
    "\n",
    "1. What behaviour do you observe as you increase regularization (that is, as $C$ becomes small)?\n",
    "\n",
    "2. From your investigation, choose the best regularization constant. How does it compare with previous testing accuracy?\n",
    "\n",
    "3. For which values of $C$ can you recover your previous training accuracy? Can you explain why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e-06 2.05061224e-04 4.09122449e-04 6.13183673e-04\n",
      " 8.17244898e-04 1.02130612e-03 1.22536735e-03 1.42942857e-03\n",
      " 1.63348980e-03 1.83755102e-03 2.04161224e-03 2.24567347e-03\n",
      " 2.44973469e-03 2.65379592e-03 2.85785714e-03 3.06191837e-03\n",
      " 3.26597959e-03 3.47004082e-03 3.67410204e-03 3.87816327e-03\n",
      " 4.08222449e-03 4.28628571e-03 4.49034694e-03 4.69440816e-03\n",
      " 4.89846939e-03 5.10253061e-03 5.30659184e-03 5.51065306e-03\n",
      " 5.71471429e-03 5.91877551e-03 6.12283673e-03 6.32689796e-03\n",
      " 6.53095918e-03 6.73502041e-03 6.93908163e-03 7.14314286e-03\n",
      " 7.34720408e-03 7.55126531e-03 7.75532653e-03 7.95938776e-03\n",
      " 8.16344898e-03 8.36751020e-03 8.57157143e-03 8.77563265e-03\n",
      " 8.97969388e-03 9.18375510e-03 9.38781633e-03 9.59187755e-03\n",
      " 9.79593878e-03 1.00000000e-02]\n"
     ]
    }
   ],
   "source": [
    "values = np.linspace(1e-6, 1e-2)\n",
    "acc_test = []\n",
    "acc_train = []\n",
    "print(values)\n",
    "for c in values:\n",
    "    model = LogisticRegression(penalty='l2', C=c)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred_test = model.predict(X_test)\n",
    "    acc_test.append(accuracy_score(Y_test, Y_pred_test))\n",
    "    \n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    acc_train.append(accuracy_score(Y_train, Y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Regularization constant for highest accuracy is: 0.00897969387755102 with 0.7662337662337663\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(values, acc_test)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('C')\n",
    "print('Test: Regularization constant for highest accuracy is:', values[np.argmax(acc_test)], 'with', np.max(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Regularization constant for highest accuracy is: 0.01 with 0.7752442996742671\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmL0lEQVR4nO3deXiV9Zn/8fdNQtg32fdFWQQFgQiC4lot1oWp1hbcascNt2nt2Na206p0evVX244zVUdH7bSiAipWRatFxqVubAmLkLCFLQkohCVhCWS9f3+cQ3sMBziBPHmScz6v6zoX51lzf0lyPnm+32cxd0dERKSmJmEXICIiDZMCQkRE4lJAiIhIXAoIERGJSwEhIiJxKSBERCSuQAPCzCaa2RozyzOz++Msf8TMlkVfa82sOGbZw2aWY2arzOz3ZmZB1ioiIl+WHtSOzSwNeBy4GCgEFpvZHHfPPbSOu98bs/49wMjo+/HA2cDw6OKPgfOAD4709Tp16uT9+vWr20aIiCS57OzsHe7eOd6ywAICGAPkufsGADObBUwCco+w/hTggeh7B5oDGYABTYFtR/ti/fr1Iysrqw7KFhFJHWa2+UjLguxi6gkUxEwXRucdxsz6Av2B9wDcfT7wPvB59DXX3VcFWKuIiNTQUAapJwOz3b0KwMxOAU4FehEJlQvNbELNjczsNjPLMrOsoqKiei1YRCTZBRkQW4DeMdO9ovPimQzMjJn+OrDA3fe5+z7gbWBczY3c/Sl3z3T3zM6d43ahiYjIcQoyIBYDA82sv5llEAmBOTVXMrMhQAdgfszsfOA8M0s3s6ZEBqjVxSQiUo8CCwh3rwTuBuYS+XB/yd1zzGyamV0Zs+pkYJZ/+bays4H1wApgObDc3d8IqlYRETmcJcvtvjMzM11nMYmI1I6ZZbt7ZrxlDWWQWkREGhgFhIhII1VeWc3ry7YwY2F+IPsP8kI5EREJwNbiA8xYmM+sxfns2FfOqD7tmTKmN3V9RyIFhIhII+DuzF+/k+nzNzNv1Taq3bloSBduGNePCad0qvNwAAWEiEi9e3FxPr9/N4+yyuqEt6moqqbkQAUdWjbl1gkDuG5sH3qf1DLAKhUQIiL1pqra+dVbq3jm442M7tuBwd3a1Gr7UX06cPnw7jRvmhZQhV+mgBARqQd7D1bw3VnLeG/1dm4a349/u+xU0tMa9nlCCggRkYAV7CrllmezyCvaxy/+6TRuOKtv2CUlRAEhIhKgrE27uP25bCqqqnn2O2M4Z2CnsEtKmAJCRCQgf135Bf8ycyk92jfnDzedycmdW4ddUq0oIEREApC7dQ/fe3EpQ3u05Y83nUmHVhlhl1RrDXuERESkESouLef257No16IpT904ulGGA+gIQkSkTlVVO/fMXMoXJQd58fZxdGnTPOySjpsCQkSkDv32nTV8tG4Hv7rqdEb16RB2OSdEXUwiInXkrRWf88QH65kypg9TxvQJu5wTpoAQEakDa7ft5b6XlzOyT3sevHJo2OXUCQWEiMgJKjlQwW3Ts2iZkc4T142mWXr93AojaBqDEBGJ8dG6Ih54PYe9ZZUJb3OwoooD5VXMvO0surVrvIPSNSkgRESips/fxENv5NK/Uyu+cmrXWm178dAunNnvpIAqC4cCQkRSXmVVNdPezGX6/M1cNKQL/zVlJK2b6eNR/wMiktJKDlRw94wlfLRuB7efO4AfThxCWpO6f/hOY6SAEJGUtWnHfm5+djH5u0p5+OrhfPPM3mGX1KAoIEQkJX1WWMyN/7sIA567eSxnDegYdkkNTqCnuZrZRDNbY2Z5ZnZ/nOWPmNmy6GutmRXHLOtjZu+Y2SozyzWzfkHWKiKpo2hvGbdNz6ZVRjqv3XW2wuEIAjuCMLM04HHgYqAQWGxmc9w999A67n5vzPr3ACNjdjEd+KW7zzOz1kDiD28VETmCiqpq7nphCcUHynnljvH07dgq7JIarCCPIMYAee6+wd3LgVnApKOsPwWYCWBmQ4F0d58H4O773L00wFpFJEX88i+rWLRpF7++ejjDerQLu5wGLciA6AkUxEwXRucdxsz6Av2B96KzBgHFZvZnM1tqZr+JHpGIiBy3V7IL+dOnm7jlnP5MOiPux5HEaCi32pgMzHb3quh0OjABuA84ExgA3FRzIzO7zcyyzCyrqKiovmoVkUZoRWEJP3l1BeNP7sj9lw4Ju5xGIciA2ALEnjPWKzovnslEu5eiCoFl0e6pSuA1YFTNjdz9KXfPdPfMzp07103VIpJ0du4rY+rz2XRq3YxHp4wkPa2h/G3csAX5v7QYGGhm/c0sg0gIzKm5kpkNAToA82ts297MDn3qXwjk1txWRORYKququXvGUor2lfHk9aPp2LpZ2CU1GoGdxeTulWZ2NzAXSAP+191zzGwakOXuh8JiMjDL3T1m2yozuw9418wMyAaeDqpWEUk+1dXOh+uKeOajjczfsJPfXTOC03tpULo2LOZzuVHLzMz0rKyssMsQkZCVlFbwcnYBzy/YzKadpXRqncEd55/Czef0D7u0BsnMst09M94yXUktIqEpLi1naX4xzon/oVpZ5by3ejuvLdvCwYpqRvftwL0XD+LS07qTka4xh+OhgBCRereisITp8zcxZ/lWyirr7hrY5k2b8E9n9OSGcX11jUMdUECISL0oq6zirRWfM33+ZpbmF9OiaRpXjerFpDN60KJp3Vzm1K9TK9q1aFon+xIFhIgEbEvxAV5YsJkXFxewc385Azq14ueXD+Xq0b30Yd7AKSBEpM65O5/k7eTZ+Zt4d9U2AC46tSs3juvL2Sd3oomet9AoKCBEGriDFVU0r0UXzIHyKvaWVcRd1r5FRq0GbHfvL6eiOvExgsoqZ27OFzy3YDMbivZzUqsMpp53MteO7UOvDi0T3o80DAoIkQaqoqqah97IYcbC/IT++l65pYRnPz36wG/7lk351pm9uX5sX3qfFP8Du7yymrdXRsYKsjfvPq7az+jdnke+NYKvnd6dZum6jVpjpYAQaYBKSiu4c0Y2n+Tt5JKhXcnevJt5udsY0KkV15/Vl29k9qJt86ZHHPgd1qPtYft04NO8HTzz0Uae+nADFw7uwg3j+nLuwM40aWJ8XnKAGQvzmbmogB37yujXsSX3XTKI9i0zalX7iF7tdUFaktCFciL16JO8Hew9WMFFp3al6RHuB7ShaB+3PJtFwe5SfnXVcL4xuhdllVW8veILps/fxJJoEFwwpDMLN+z6+8DvDeP6cvXoSHAczT+CIJ8d+8rp17Elp3Rpw/trtlPtflhwSHI72oVyCgiRepK9eReTn1pARZXTtW0zrh3Tlylje9OlTfO/r/Np3g7ueGEJaU2M/7lhNGf2O+mw/azcErmGYG7ONsb0P+m4B35ju5Lyd5Vy1aieR+16kuSkgBAJ2bY9B7n80Y9pmZHGjyYOYdbiAj5cW0R6E+PS07tz47i+rN22lwdez2FA51b84dtn6oNa6oVutSESovLKau54Ppv9ZZU8f/NYBndrw9dO787GHft5fsFmXs4q4I3lWwG4YHBnfj9lJG2O0U0kUh8UECIBe+iNHJbkF/PYtSMZ3K3N3+f379SKn10+lH+9ZBBzlm2l5EAFt0wYQJr6/aWBUECIBOjFxfm8sDCf288bwOXDe8Rdp2VGOpPH9KnnykSOTbc4FAnIsoJifvZaDhMGduKHX9UjLqXxUUCIBKBobxlTn8umS9tm/H7ySHUbSaOkLiaROlZd7dw9YwnFB8p55Y7xdGhVuwvNRBoKBYRIHXsxq4CFG3fx8DeG65kE0qipi0mkDhWXlvPwX1czpv9JXDO6V9jliJwQBYRIHfrN3DXsOVjJtEnDMNO4gzRuCgiROrKisIQZi/K5cVxfhnQ7/GZ5Io2NAkKkDlRXOz+fs5KOrTL43lcGhV2OSJ1QQIjUgdlLClmaX8z9l56qx2hK0gg0IMxsopmtMbM8M7s/zvJHzGxZ9LXWzIprLG9rZoVm9liQdYqciJLSCn799mpG9+3AVSN7hl2OSJ0J7DRXM0sDHgcuBgqBxWY2x91zD63j7vfGrH8PMLLGbn4BfBhUjSJ14T/mrWF3aTnTJ43R8xMkqQR5BDEGyHP3De5eDswCJh1l/SnAzEMTZjYa6Aq8E2CNIickZ2sJzy3YzPVn9dU1D5J0ggyInkBBzHRhdN5hzKwv0B94LzrdBPgdcF+A9YmcEHfngddz6NAyg3+9eHDY5YjUuYYySD0ZmO3uVdHpO4G33L3waBuZ2W1mlmVmWUVFRYEXKXLIwYoq/mXWMrI27+ZHE4fQrqUGpiX5BHmrjS1A75jpXtF58UwG7oqZHgdMMLM7gdZAhpntc/cvDXS7+1PAUxB5olxdFS5yNNv3HuS26dksKyjmhxMHc02mrpiW5BRkQCwGBppZfyLBMBm4tuZKZjYE6ADMPzTP3a+LWX4TkFkzHETCkLO1hFufzWJ3aQVPXj+aiad1C7skkcAE1sXk7pXA3cBcYBXwkrvnmNk0M7syZtXJwCxPlodjS9Kam/MF33hiPg68PHWcwkGSniXL53JmZqZnZWWFXYY0AhVV1ew5UFGrbV7KKuThuasZ3qs9T98wmi5tmwdUnUj9MrNsd8+Mt0y3+5aUsnJLCbc/l82W4gO13vby4d357TUjaN40LYDKRBoeBYSkjL+u/IJ7X1xG+5ZN+fnlQ0lPS/yitk6tm3Hpad10h1ZJKQoISXruzhN/W8/Df13DiN7tefrG0XRpoy4ikWNRQEhSK6us4sd/XsGfl2zhihE9+M03hquLSCRBCghJWjv2lTH1uWyyNu/m+xcP4p4LT1EXkUgtKCAkKRXtLeOqJz6haG8Zj187isuGdw+7JJFGRwEhSaeiqpq7XlhC0d4yZt56FiP7dAi7JJFGSQEhSeeXf1nFok27+K/JZygcRE5AQ7lZn0idmJ1dyJ8+3cStE/oz6Qw9vEfkRCggJGmsKCzhJ6+uYPzJHfnRxCFhlyPS6CkgJCns3FfG7c9l0bl1Mx6dMpL0NP1oi5wojUFIo1dZVc1dM5awc385r9wxno6tm4VdkkhSUEBIo/ert1ezYMMu/uObIzitpx77KVJXdBwujdq7q7bxh483ctP4flw1Sg/uEalLCghptA5WVPHgGzmc0qU1P/naqWGXI5J01MUkjdb//G0DBbsOMOOWsWSk628dkbqm3ypplAp2lfLfH+Rx2fDujD+lU9jliCSlYwaEmV1hZgoSaVCmvZlLWhPj3y5T15JIUBL54P8WsM7MHjYzXX0koXt/zXbm5W7jngsH0r1di7DLEUlaxwwId78eGAmsB/5kZvPN7DYzaxN4dSI1lFVW8dCcHAZ0bsXN5/QPuxyRpJZQ15G77wFmA7OA7sDXgSVmdk+AtYkc5pmPNrJpZykPXjFMA9MiAUtkDOJKM3sV+ABoCoxx90uBEcC/BlueyD9sKT7Ao++tY+Kwbpw7qHPY5YgkvUROc70aeMTdP4yd6e6lZnZzMGWJHO7f38wF4GdXDA25EpHUkMgx+oPAokMTZtbCzPoBuPu7R9vQzCaa2RozyzOz++Msf8TMlkVfa82sODr/jOhYR46ZfWZm36pFmyQJfbxuB2+v/IK7LziFnu01MC1SHxI5gngZGB8zXRWdd+bRNjKzNOBx4GKgEFhsZnPcPffQOu5+b8z69xAZDAcoBW5093Vm1gPINrO57l6cQL2SZNyd381bQ8/2Lbj13AFhlyOSMhI5gkh39/JDE9H3GQlsNwbIc/cN0W1mAZOOsv4UYGb0a6x193XR91uB7YA6nVPUgg27WJpfzNTzBtAsPS3sckRSRiIBUWRmVx6aMLNJwI4EtusJFMRMF0bnHcbM+gL9gffiLBtDJJDWx1l2m5llmVlWUVFRAiVJY/TfH+TRqXUzrsnsHXYpIiklkYCYCvzEzPLNrAD4EXB7HdcxGZjt7lWxM82sO/Ac8B13r665kbs/5e6Z7p7ZubMOMJLR8oJiPlq3g1sn9Kd5Ux09iNSnY45BuPt64Cwzax2d3pfgvrcAsX/y9YrOi2cycFfsDDNrC/wF+Km7L0jwa0qSefz9PNo2T+e6s/qGXYpIyknobq5mdhkwDGhuZgC4+7RjbLYYGGhm/YkEw2Tg2jj7HgJ0AObHzMsAXgWmu/vsRGqU5LN2217eyd3Gv1w0kNbNdONhkfqWyIVyTxK5H9M9gAHXAMf8c87dK4G7gbnAKuAld88xs2mxYxpEgmOWu3vMvG8C5wI3xZwGe0aCbZIk8cQH62mZkcZ3xvcLuxSRlGRf/lyOs4LZZ+4+PObf1sDb7j6hfkpMTGZmpmdlZYVdhtSR/J2lXPC7D/jns/vx08t0YZxIUMws290z4y1LZJD6YPTf0ug1CRVE7sckEpgnP1xPmhm3TNB1DyJhSaRj9w0zaw/8BlgCOPB0kEVJatu25yCzswr5RmYvurZtHnY5IinrqAERfVDQu9ErmF8xszeB5u5eUh/FSWp65qMNVLkz9dyTwy5FJKUdtYspeu3B4zHTZQoHCdLu/eW8sDCfK0f0oE/HlmGXI5LSEhmDeNfMrrZD57eKBOiPn2yktLyKO87X0YNI2BIJiNuJ3JyvzMz2mNleM9sTcF2SgrYUH+CpjzZw2endGdRVDywUCVsiV1LrN1XqxaHnPfzkslNDrkREIIGAMLNz482v+QAhkRPx0boi3l75BfddMkjPexBpIBI5zfUHMe+bE7mNdzZwYSAVScopr6zmgTk59OvYUs97EGlAEuliuiJ22sx6A/8ZVEGSev73k41sKNrPH79zpp73INKAJDJIXVMhoE5iqROflxzg9++u4+KhXblgcJewyxGRGImMQTxK5OppiATKGUSuqBY5Yb/8yyqqqp2fX677LYk0NImMQcTeAa8SmOnunwRUj6SQT9fv4M3PPud7XxlI75N0UZxIQ5NIQMwGDh562puZpZlZS3cvDbY0SWYVVdU88HoOvU9qwdTzdFGcSEOU0JXUQOx5hy2A/wumHEkVf/pkE+u27+OBy4fpUaIiDVQiAdE89jGj0ffqD5Dj9nJWAQ/PXc2FQ7rwlaFdwy5HRI4gkYDYb2ajDk2Y2WjgQHAlSbKqqnZ+9dYqfjD7M8b278gj3zwj7JJE5CgSGYP4HvCymW0l8sjRbkQeQSqSsP1llXx31jL+b9U2bjirLz+/YihN047nLGsRqS+JXCi32MyGAIOjs9a4e0WwZUky2VJ8gJv/tJh12/cxbdIwbhzXL+ySRCQBx/wTzszuAlq5+0p3Xwm0NrM7gy9NkkH25t1MeuxjthQf4I83nalwEGlEEjnGvzX6RDkA3H03cGtgFUnSyNu+l2ufXkCrZum8eud4zh3UOeySRKQWEhmDSDMzc3eHyHUQQEawZUlj5+48MCeHZulNeHnqOLq00bOlRRqbRI4g/gq8aGYXmdlFwEzg7WDLksburRVf8EneTu776mCFg0gjlUhA/Ah4D5gafa3gyxfOHZGZTTSzNWaWZ2b3x1n+iJkti77WmllxzLJvm9m66OvbCbVGGoT9ZZX8+19yGdq9LdeN7Rt2OSJynBI5i6nazBYCJwPfBDoBrxxru2hX1OPAxUTuALvYzOa4e27Mvu+NWf8eYGT0/UnAA0AmkRsFZke33V2LtklIHns/j89LDvLolJGkNdGjzEUaqyMeQZjZIDN7wMxWA48C+QDufoG7P5bAvscAee6+wd3LgVnApKOsP4VI9xXAV4F57r4rGgrzgIkJfE0J2YaifTzz0QauGtWTzH4nhV2OiJyAo3UxrSby1LjL3f0cd38UqKrFvnsCBTHThdF5hzGzvkB/Il1ZCW9rZreZWZaZZRUVFdWiNAmCu/PgG7k0T0/jx5fqkSEijd3RAuIq4HPgfTN7OjpAHVR/wWRg9qE7xibK3Z9y90x3z+zcWadQhm1uzjY+XFvEvRcPonObZmGXIyIn6IgB4e6vuftkYAjwPpFbbnQxsyfM7JIE9r0F6B0z3Ss6L57J/KN7qbbbSgNwoLyKX7yZy5BubbhxnAamRZLBMc9icvf97j4j+mzqXsBSImc2HctiYKCZ9TezDCIhMKfmStHbeHQA5sfMngtcYmYdzKwDcEl0njRQT3yQx5biAzx05TDSdY8lkaRQq99kd98d7da5KIF1K4G7iXywrwJecvccM5tmZlfGrDoZmHXoQrzotruAXxAJmcXAtOg8aYA279zPkx9uYNIZPRg7oGPY5YhIHbGYz+VGLTMz07Oyso69otS5f/7TYhZu2Ml7951P17a6KE6kMTGzbHfPjLdMfQFyQv4vdxvvrd7O974ySOEgkmQUEHLcDlZU8dCbOQzs0pqbzu4XdjkiUscSuVmfSFxP/m09BbsOMOPWsXr4j0gS0m+1HJeCXaU88cF6Lh/enfEndwq7HBEJgAJCjsu0N3NJa2L89DJdMS2SrBQQUmvvr9nOvNxt3HPhQLq3S+jGviLSCCkgpFbKKqt4aE4OAzq34uZz+oddjogESIPUUitPf7iBTTtLee7mMWSk6+8LkWSm33BJWOHuUh57P49LT+vGhIG6OaJIslNASML+Y95aAP7t8qEhVyIi9UEBIQkp2FXK68u2ct3YvvRsr4FpkVSggJCEPPXhBtLMuHXCgLBLEZF6ooCQY9q+5yAvZhVw9ehedGun+y2JpAoFhBzTHz7eSGVVNVPP09GDSCpRQMhRFZeW8/yCzVwxogd9O7YKuxwRqUcKCDmqZz/dzP7yKu48/5SwSxGReqaAkCPaX1bJHz/dyMVDuzK4W5uwyxGReqaAkCOasTCf4tIK7jz/5LBLEZEQKCAkroMVVTz90QbOPqUjI/t0CLscEQmBAkLimp1dyPa9ZdylsQeRlKWAkMNUVlXz5N/WM7JPe8ad3DHsckQkJLqbawrbtGM/n5ccPGz+kvzdFO4+wINXDMPMQqhMRBoCBUSKKimt4Gu//4jS8qq4y4d0a8OFQ7rUc1Ui0pAEGhBmNhH4LyANeMbd/1+cdb4JPAg4sNzdr43Ofxi4jEg32Dzgu+7uQdabSl5dWkhpeRWPfGsE3doefvO9gV1b06SJjh5EUllgAWFmacDjwMVAIbDYzOa4e27MOgOBHwNnu/tuM+sSnT8eOBsYHl31Y+A84IOg6k0l7s7MRQUM79WOr4/sFXY5ItJABTlIPQbIc/cN7l4OzAIm1VjnVuBxd98N4O7bo/MdaA5kAM2ApsC2AGtNKUsLilmzbS9TxvQJuxQRacCCDIieQEHMdGF0XqxBwCAz+8TMFkS7pHD3+cD7wOfR11x3X1XzC5jZbWaWZWZZRUVFgTQiGc1cmE+rjDSuGNEj7FJEpAEL+zTXdGAgcD4wBXjazNqb2SnAqUAvIqFyoZlNqLmxuz/l7pnuntm5sx6BmYg9Byt447OtXHlGT1o30zkKInJkQQbEFqB3zHSv6LxYhcAcd69w943AWiKB8XVggbvvc/d9wNvAuABrTRmvL93CwYpqpozpfeyVRSSlBRkQi4GBZtbfzDKAycCcGuu8RuToATPrRKTLaQOQD5xnZulm1pTIAPVhXUxSO+7OCwvzGdajLaf3bBd2OSLSwAUWEO5eCdwNzCXy4f6Su+eY2TQzuzK62lxgp5nlEhlz+IG77wRmA+uBFcByIqe/vhFUralieWEJq7+IDE7rAjgROZZAO6Hd/S3grRrzfh7z3oHvR1+x61QBtwdZWyqauTCfFk3TmHSGBqdF5NjCHqSWerL3YAVzlm/lyhE9aNO8adjliEgjoIBIEa8v28qBiiqmjNW1DyKSGAVEipi1OJ9Tu7dlRC8NTotIYhQQKWBFYQkrt+zh2jG9NTgtIglTQKSAGYvyad60CZNG1ryQXUTkyBQQSW5fWSVzlm3h8uE9aKvBaRGpBQVEkntj+Vb2l1fpymkRqTUFRJKbuSifwV3bMKpPh7BLEZFGRgGRxFZuKeGzwhKmaHBaRI6DAiKJzVyUT7P0JnookIgcFwVEktpfVsnry7Zy2fDutGupwWkRqT0FRJJ687Ot7Cur5Fo9NU5EjpMCIknNWFTAwC6tGd1Xg9MicnwUEEkoZ2sJywuKdVtvETkhCogkNGtRARnpTbhqlK6cFpHjp4BIMqXllby2dAtfO60b7VtmhF2OiDRiCogk8+Znn7O3rJIpGpwWkROkgEgysxblM6BzK8b0PynsUkSkkVNAJJHVX+xhSX4x12pwWkTqgAIiicxaVEBGWhOuGqUrp0XkxCkgksSB8ir+vKSQiad146RWGpwWkROngEgSrywpZM/BSibrtt4iUkcUEEmguLSc372zhjH9TmLcgI5hlyMiSSLQgDCziWa2xszyzOz+I6zzTTPLNbMcM5sRM7+Pmb1jZquiy/sFWWtj9tt31rDnYCUPTRqmwWkRqTPpQe3YzNKAx4GLgUJgsZnNcffcmHUGAj8Gznb33WbWJWYX04Ffuvs8M2sNVAdVa2O2cksJLyzM59vj+nFq97ZhlyMiSSTII4gxQJ67b3D3cmAWMKnGOrcCj7v7bgB33w5gZkOBdHefF52/z91LA6y1Uaqudn72+ko6tsrg3osHhV2OiCSZIAOiJ1AQM10YnRdrEDDIzD4xswVmNjFmfrGZ/dnMlprZb6JHJF9iZreZWZaZZRUVFQXSiIZs9pJCluYXc/+lp9KuhZ75ICJ1K+xB6nRgIHA+MAV42szaR+dPAO4DzgQGADfV3Njdn3L3THfP7Ny5cz2V3DCUHKjg12+vZlSf9lw1UjflE5G6F2RAbAFiz7nsFZ0XqxCY4+4V7r4RWEskMAqBZdHuqUrgNWBUgLU2Oo/MW8vu0nKmTTqNJk00MC0idS/IgFgMDDSz/maWAUwG5tRY5zUiRw+YWSciXUsbotu2N7NDhwUXArkIALlb9zB9/iauG9uX03q2C7scEUlSgQVE9C//u4G5wCrgJXfPMbNpZnZldLW5wE4zywXeB37g7jvdvYpI99K7ZrYCMODpoGptTNydB+aspH3LDO67ZHDY5YhIEgvsNFcAd38LeKvGvJ/HvHfg+9FXzW3nAcODrK8xmrEon8WbdvPrq0+nXUsNTItIcAINCKk77s5j7+Xxu3lrOfuUjlwzWrfUEJFgKSAagYMVVfxw9mfMWb6Vq0b15FdXna6BaREJnAKigdu+9yC3Tc9mWUExP5w4mDvOO1m30xCReqGAaMByt+7hlmcXs7u0gievH83E07qFXZKIpBAFRANUWVXNG59t5aevrqRdi6a8PHWcTmcVkXqngGhAivaW8eLifGYszGdryUFG9G7P0zeMpkvb5mGXJiIpSAERMndnSX4x0+dv4q0Vn1NR5ZxzSiceuHIYFw3pQnpa2HdDEZFUlfIBUVxazjVPzg/t6x+oqKJw9wHaNEvnurF9uWFcX07u3Dq0ekREDkn5gGjSxBjYNbwPZDNj6nkn8/WRPWnVLOW/HSLSgKT8J1Lb5k357+tGh12GiEiDow5uERGJSwEhIiJxKSBERCQuBYSIiMSlgBARkbgUECIiEpcCQkRE4lJAiIhIXBZ56mfjZ2ZFwOYT2EUnYEcdldNYpFqbU629oDanihNpc1937xxvQdIExIkysyx3zwy7jvqUam1OtfaC2pwqgmqzuphERCQuBYSIiMSlgPiHp8IuIASp1uZUay+ozakikDZrDEJEROLSEYSIiMSVlAFhZhPNbI2Z5ZnZ/XGWNzOzF6PLF5pZv5hlP47OX2NmX010n2Gr6zabWW8ze9/Mcs0sx8y+W4/NSUgQ3+fosjQzW2pmb9ZDM2oloJ/t9mY228xWm9kqMxtXT81JSEBtvjf6c73SzGaaWYN58PvxttfMOkZ/Z/eZ2WM1thltZiui2/zezCyhYtw9qV5AGrAeGABkAMuBoTXWuRN4Mvp+MvBi9P3Q6PrNgP7R/aQlss8kbHN3YFR0nTbA2mRvc8x23wdmAG+G3c76aDPwLHBL9H0G0D7stgb8s90T2Ai0iK73EnBT2G2tg/a2As4BpgKP1dhmEXAWYMDbwKWJ1JOMRxBjgDx33+Du5cAsYFKNdSYR+aUAmA1cFE3UScAsdy9z941AXnR/iewzTHXeZnf/3N2XALj7XmAVkV+shiKI7zNm1gu4DHimHtpQW3XeZjNrB5wL/AHA3cvdvTj4piQskO8zkadptjCzdKAlsDXgdiTquNvr7vvd/WPgYOzKZtYdaOvuCzySFtOBf0qkmGQMiJ5AQcx0IYd/sP19HXevBEqAjkfZNpF9himINv9d9BB2JLCwLos+QUG1+T+BHwLVdV7xiQuizf2BIuCP0W61Z8ysVTDlH5c6b7O7bwF+C+QDnwMl7v5OINXX3om092j7LDzGPuNKxoCQOmRmrYFXgO+5+56w6wmSmV0ObHf37LBrqUfpwCjgCXcfCewHGtwYW10ysw5E/grvD/QAWpnZ9eFW1TAlY0BsAXrHTPeKzou7TvQQsx2w8yjbJrLPMAXRZsysKZFweMHd/xxI5ccviDafDVxpZpuIHNpfaGbPB1H8cQqizYVAobsfOjqcTSQwGoog2vwVYKO7F7l7BfBnYHwg1dfeibT3aPvsdYx9xhf2oEwAgzzpwAYifx0cGuQZVmOdu/jyIM9L0ffD+PKg1gYig0bH3GcSttmI9FX+Z9jtq68219j2fBreIHUgbQY+AgZH3z8I/Cbstgb8sz0WyCEy9mBE+vPvCbutJ9remOU3cexB6q8lVE/Y/yEB/Sd/jchZN+uBn0bnTQOujL5vDrxMZNBqETAgZtufRrdbQ8xIf7x9NqRXXbeZyNkQDnwGLIu+EvqhaqxtrrHv82lgARHgz/YZQFb0e/0a0CHsdtZDmx8CVgMrgeeAZmG3s47auwnYBewjcnQ4NDo/M9rW9cBjRC+SPtZLV1KLiEhcyTgGISIidUABISIicSkgREQkLgWEiIjEpYAQEZG4FBAiATKzbmY2y8zWm1m2mb1lZoPCrkskEelhFyCSrKI3jHsVeNbdJ0fnjQC6EjnPXaRBU0CIBOcCoMLdnzw0w92Xh1iPSK2oi0kkOKcBqXTjP0kyCggREYlLASESnBxgdNhFiBwvBYRIcN4DmpnZbYdmmNlwM5sQYk0iCVNAiATEI3fC/DrwlehprjnAr4Avwq1MJDG6m6uIiMSlIwgREYlLASEiInEpIEREJC4FhIiIxKWAEBGRuBQQIiISlwJCRETiUkCIiEhc/x/HVt10EmxcvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(values, acc_train)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('C')\n",
    "print('Train: Regularization constant for highest accuracy is:', values[np.argmax(acc_train)], 'with', np.max(acc_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What behaviour do you observe as you increase regularization (that is, as $C$ becomes small)?\n",
    "\n",
    "\n",
    "Ans: As C becomes small, accuracy seems to peak around 76.6% around 0.0089 and then drops later with much stronger regularization\n",
    "\n",
    "2. From your investigation, choose the best regularization constant. How does it compare with previous testing accuracy?\n",
    "\n",
    "The best regularization constant is 0.01 based on a high accuracy of 77.5%. It is slightly higher than the original testing accuracy of 75.9%.\n",
    "\n",
    "3. For which values of $C$ can you recover your previous training accuracy? Can you explain why does this happen?\n",
    "\n",
    "The training accuracy was 77.68%. The maximum accuracy is 77.5% with regularization. When you apply regularization, you are essentially introducing constraints on the model parameters. This can prevent the model from fitting the training data too closely and capturing noise, which might result in a reduction of training accuracy. However, the primary goal is to improve the model's performance on new, unseen data. So, only with no regularization can we recover the previous training accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWRSCSFf--br"
   },
   "source": [
    "# Probability Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv_JxY-R_n0r"
   },
   "source": [
    "Recall that we are not estimating $Y$ directly, but instead the probability of $Y = 1 | X$. So far, our classification has been based on the which class has the higher probability. In other words, we have been using the classifier:\n",
    "$$\n",
    "  \\hat{Y}(x) =\n",
    "    \\begin{cases}\n",
    "      0 & \\text{if } \\hat{p}(x) < 0.5 \\\\\n",
    "      1 & \\text{if } \\hat{p}(x) \\geq 0.5 \\\\\n",
    "    \\end{cases}       \n",
    "$$\n",
    "\n",
    "However, we must also consider that the probability threshold of 0.5 might not be optimal. Indeed, consider our current data-set. Let's say we were going to use our model to decide who should go see a doctor and possibly get preventive treatment. In this case, incorrectly predicting that someone will not have Diabetes, when they will, is very costly - the person might face severe health consequences. On the other hand, incorrectly saying someone will have Diabetes, when they won't, is not as bad because the doctor would be able to catch this, and we would just waste a little time. \n",
    "\n",
    "This is one of the greatest strengths of logistic regression. The way we define and train them, mean we obtain good uncertainty estimates, so that we might make risk assessments!\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "1. Train a new logistic regression model, using the best regularization constant you found in exercise 2.\n",
    "\n",
    "2. Build a new function which takes as inputs: your Logistic Regression model, a set of $X$ values in which to predict, and a probability threshold $T_p$. The function should return an array of predictions based on the new probability threshold. (Hint:  use the $\\texttt{predict\\_proba}$ method in $\\texttt{LogisticRegression}$)\n",
    "\n",
    "3. Investigate the effect of the probability threshold on the number of False Negatives and False Positives. Based on this analysis, what probability threshold would YOU choose? What kind of information would you want to make such a decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.00897969387755102)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.00897969387755102)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.00897969387755102)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Train a new logistic regression model, using the best regularization constant you found in exercise 2.\n",
    "max_test_c = 0.00897969387755102\n",
    "model2 = LogisticRegression(penalty='l2', C=max_test_c)\n",
    "model2.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Build a new function which takes as inputs: your Logistic Regression model, a set of $X$ values in which to predict, and a probability threshold $T_p$. The function should return an array of predictions based on the new probability threshold. (Hint:  use the $\\texttt{predict\\_proba}$ method in $\\texttt{LogisticRegression}$)\n",
    "def predict(model, X, Tp):\n",
    "    y_pred = model.predict_proba(X)\n",
    "    y = map(lambda x: 1 if x[1] >= Tp else 0, y_pred)\n",
    "    return list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.4 False negative: 21 False positive:  19\n",
      "Threshold: 0.5 False negative: 28 False positive:  8\n",
      "Threshold: 0.6 False negative: 38 False positive:  2\n",
      "Threshold: 0.7 False negative: 51 False positive:  1\n",
      "Threshold: 0.8 False negative: 54 False positive:  0\n",
      "Threshold: 0.9 False negative: 54 False positive:  0\n"
     ]
    }
   ],
   "source": [
    "#3. Investigate the effect of the probability threshold on the number of False Negatives and False Positives. \n",
    "#Based on this analysis, what probability threshold would YOU choose? \n",
    "#What kind of information would you want to make such a decision?\n",
    "\n",
    "Tps = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for thresh in Tps:\n",
    "    y_pred = predict(model, X_test, thresh)\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test, y_pred).ravel()\n",
    "    print('Threshold:', thresh, 'False negative:', fn, 'False positive: ', fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower Threshold (< 0.5):\n",
    "Decreasing the threshold increases the sensitivity, reducing false negatives but potentially increasing false positives.\n",
    "\n",
    "Higher Threshold (> 0.5):\n",
    "Increasing the threshold increases specificity, reducing false positives but potentially increasing false negatives.\n",
    "\n",
    "Making a Decision:\n",
    "To choose a probability threshold, you need to consider the specific goals and requirements of your problem:\n",
    "\n",
    "False Positives vs. False Negatives Trade-off:\n",
    "\n",
    "If false positives are more costly than false negatives (e.g., in medical diagnosis), you might choose a higher threshold to reduce false positives.\n",
    "If false negatives are more costly (e.g., in fraud detection), you might choose a lower threshold to reduce false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IMP-PCMLAI-M13-LogisticRegressionNotebook-final.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "d37abda7630e259e5026a5079657683a09f6e3d11473720762ebe7250c494840"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
